<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Sapient - CCTV Analysis AI</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/grayscale.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="index.html">SAPIENT</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="requirements.html">Requirements</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="research.html">Research</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="hci.html">HCI</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="prototype.html">Prototype</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container d-flex h-100 align-items-center">
        <div class="mx-auto text-center">
          <h1 class="mx-auto my-0 text-uppercase">Prototype</h1>
          <h2 class="text-white-50 mx-auto mt-2 mb-5">.</h2>
          <a href="#about" class="btn btn-primary js-scroll-trigger">Show me more</a>
        </div>
      </div>
    </header>

    <!-- Black Spot Beneath Header Section -->
    <section id="about" class="about-section text-center">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h1 class="text-white mb-4 text-uppercase"></h1>
            <p class="text-white-50" style="text-align: justify;"></p>
          </div>
        </div>
      </div>
    </section>
    
    <!-- Review Existing Section -->
    <section id="project-background" class="projects-section bg-light">
      <div class="container">
        <div class="col-lg-12 text-center">
          <h1 class="section-heading text-uppercase">System architecture diagram</h1>
          <p class="line"></p>
          <p class="text-black-100" style="text-align: justify;" >We have three separate systems to present our prototype:</p>


          <center>
            <img class="sketch" src="img/systemArchitecture.png">
          </center>

          <br>
          <ul>
            <li><h3 align="left">Azure VM:</h3> <p class="text-black-100" style="text-align: justify;">It is the virtual machine from Azure Cloud Platform which gathers APIs, video and webapplication all together in order to process them in one box.</p></li>
            <br>
            <li><h3 align="left">Video:</h3> <p class="text-black-100" style="text-align: justify;">It is a directory containing video as an input to the APIs.</p></li>
            <br>
            <li><h3 align="left">Item Recognition:</h3> <p class="text-black-100" style="text-align: justify;">It is an image recognition processing package divided into object, image and face recognition. The output will be the video after being processed by the package.</p></li>
            <br>
            <li><h3 align="left">Image Recognition:</h3> <p class="text-black-100" style="text-align: justify;">It is an API which can track points from human body from recorded video, and outputs the video with the tracking lines on human body.</p></li>
            <br>
            <li><h3 align="left">Face Recognition:</h3> <p class="text-black-100" style="text-align: justify;">It is an API which can track the face from human and display the name of this person in a live stream video.</p></li>
            <br>
            <li><h3 align="left">Webapp:</h3> <p class="text-black-100" style="text-align: justify;">It is the web application which displays the video which has been processed.</p></li>
          </ul>




        </div>
      </div>
    </section>


    <!-- Summary of final Section -->
    <section id="requirements-gathering" class="projects-section bg-light">
      <br>
      <br>
      <br>
      <div class="container">
        <div class="col-lg-12 text-center">
          <h1 class="section-heading text-uppercase">Implementation of finished functionalities</h1>
          <p class="line"></p>
          <p class="text-black-100" style="text-align: justify;" >Here is the procedure of how we managed to combine the APIs, Webapp and Azure VM together in the Azure cloud platform:</p>
          <br>
          <br>
          <br>

          <div class="row"> 
            <div class="column">
              <h3 align="left"><u>ZONE TWO</u></h3>
              <img class="sketch" src="img/image-api.jpg" align="right" style="width:40%;">
              <br>
              <p class="text-black-100" style="text-align:justify;">
                  Firstly, we used openpose to analyse the images to extract x and y coordinates of 18 body joints from human. Using tensorflow to extract the data of standing, sitting and laying according to the patterns of 18 body joints. After that, it outputs a dataset which consists of records of the postures (sitting, standing and laying) with 18 body joints.
                  <br><br>
                  Secondly, we feed the datasets into MLPclassifiers/Adaboost in order to train them. After that, it outputs a model which is equivalent to a brain with the knowledge of the behaviour of sitting, standing and laying.
                  <br><br>
                  Finally, we used openpose again with the model generated to process and analyse if the input file has human who is performing sitting, standing and laying. The input file can be a JPG image, pre-saved video and live video (camera).
                  <br><br>
                  One thing that require to notice is the number of the images that we first used to train the classifiers have to be more than 10, otherwise, both MLPclassifier and Adaboost will have insufficient number of data to compare. 
                  <br><br>
              </p>

              <h3><u>ZONE ONE</u></h3>
              <img class="sketch" src="img/sketches/xxx.png" align="left">
              <p class="text-black-100" style="text-align:justify;">The same procedure of installing and configuring the API will be performed by Shun Shao 
                who is in charge of face and object recognition. Shun is also using his own machine to install and configure the APIs with the specs.
              <br>
              <br>
              The detail about the face recognition installation.
              Here are the dependencies required for face recognition:
              <br>   
                <br>- software one
                <br>- software two
              <br>
              The detail about the object recognition installation.
              Here are the dependencies required for object recognition:
              <br>
                <br>- software one
                <br>- software two
              </p>
              <br>

              <h3 align="left"><u>DASHBOARD</u></h3>
              <img class="sketch" src="img/sketches/xxx.png" align="right">
              <p class="text-black-100" style="text-align:justify;"></p>

              <h3 align="right"><u>CLOUD BASED INFRASTRUCTURE</u></h3>
              <img class="sketch" src="img/sketches/xxx.png" align="left">
              <p class="text-black-100" style="text-align:justify;">We rented a standard GPU family Azure machine with NC6 series of graphic card which sufficient for openpose and tensorflow to process image and extract data. And then, we hosted the Posture Recognition and a webapp for testing purpose into the server for the purpose of testing deployment of one recognition. 
              <br>
              <br>
              Firstly, we installed the apache HTTP server and test if the web application is functional by clicking the url of the virtual machine. Secondly, we installed the environment for the posture recognition that we developed and test if posture recognition functions properly and outputs the processed image or video by using the webapp to check. In other words, we are treating this kind of webapp as the screen of the server.
              <br>
              <br>
              Lastly, we used another Azure service called Azure Storage --Block Blob to store the media files which are classified into two folders: one consisting of lots of images to train the posture recognition's classifiers and the other consisting of images and videos to let the Posture Recognition to identify the sitting, standing and laying features.
              </p>
              <br>








            </div>
          </div>
        </div>
      </div>

    </section>


    </section>

    <!-- Footer -->
    <footer class="bg-black small text-center text-white-50">
      <div class="container">
        Copyright &copy; Sapient Platform 2018
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/grayscale.min.js"></script>

  </body>

</html>
