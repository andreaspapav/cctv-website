<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Sapient - CCTV Analysis AI</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/grayscale.min.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">
  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="index.html">SAPIENT</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="requirements.html">Requirements</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="research.html">Research</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="hci.html">HCI</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="design.html">Design</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="testing.html">Testing</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="evaluation.html">Evaluation</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="management.html">Management</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container d-flex h-100 align-items-center">
        <div class="mx-auto text-center">
          <h1 class="mx-auto my-0 text-uppercase">Design</h1>
          <h2 class="text-white-50 mx-auto mt-2 mb-5">Introducing Sapient's System Architecture</h2>
          <a href="#system_architecture" class="btn btn-primary js-scroll-trigger">Show me more</a>
        </div>
      </div>
    </header>

    <div class="container-fluid bg-light">
      <div class="row">
        <div class="col-sm-2">
          <br>
          <br>
          <div id="sidebar" class="sidebar">
           <nav class="navbar navbar-shrink">
              <ul class="navbar-nav">
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#system_architecture">System Architecture</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#page_flow">Page Flow</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#sequence_diagram">Sequence Diagram</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#design_pattern">Design Patterns</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#implementation">Implementation of key functionalities</a>
                </li>
              </ul>
            </nav>
          </div>
        </div>

        <div class="col-sm-10">

          <section id="system_architecture" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">System Architecture</h1>
                <p class="line"></p>
                <br>
                <p class="text-black-100" style="text-align: justify;" >Here we will show two system architecture for our applications:</p>
                <ul class="text-black-100" style="text-align: justify;">
                  <li>AzureVM Integration</li>
                  <li>Local Live Integration</li>
                </ul>
                <p class="text-black-100" style="text-align: justify;">The reason why we have two systems are the followings:</p>
                <ul class="text-black-100" style="text-align: justify;">
                  <li>We did not manage to bring live streaming functionality into the AzureVM while we are heavily developing our Image Recognition.</li>
                  <li>It is too expensive to analyse multiple IP cameras in real-time with Image Recognition. This can come from the network latency, GPU performance, multi-threading and concurrency.</li>
                </ul>
                <br>
                <h2 class="section-heading text-uppercase"><u>AzureVM Integration</u></h2>
                <img class="sketch" src="img/design/AzureVM_brief.png" style="width: 100%">
                <p class="text-black-100" style="text-align: justify;" ><b>- AzureVM:</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is the machine we rent from Azure, which allows us to have a powerful GPU to run our image recogitions and route the website to the world by opening port 80</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Image Recognition AI</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is a package including the posture, item and face recognition.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Posture SDK</b></p>
                <p class="text-black-100" style="text-align: justify;" >It uses openpose and tensorflow to perform the posture and human detection in a frame and machine learning of recognising standing, sitting and laying.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Face SDK</b></p>
                <p class="text-black-100" style="text-align: justify;" >It can catch human face and recognise the person with the trained model</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Item API</b></p>
                <p class="text-black-100" style="text-align: justify;" >It uses Google Vision API to recognise items surrounded.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Apache Webserver</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is the webserver which mainly runs PHP, and our web application is mainly written in PHP. In addition, it has a high compatibility to access the Linux server and modify the filesystem in order to execute the image recognition.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Object Storage</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is the directory in the Linux Filesystem, and it stores the input of images and videos from the user and the output of log files and videos from the image recognition.</p>
                <br>

                <h2 class="section-heading text-uppercase"><u>Local Live Integration</u></h2>
                <!--<img src="img/systemArchitecture.png">-->
                <p class="text-black-100" style="text-align: justify;" >
                As we can see from the video/the system diagram. We successfully implemented all of these features, some by commercial API like a plug-in. and some by free SDK like a local machine.
                For the video and image analysis, we deploy it onto the azure virtual machine which brings huge powers to analyse the video with the video and text report output, including the information like people detecting in the frame, top5 most significant items detected with confidence rate, people posture recognised.
                </p>
                <p class="text-black-100" style="text-align: justify;" >
                It is using the Apache web server to communicate with the front-end and back-end web apps, people from different countries can access the website and using the recognition by the URL we are given. However, it is only for the video and images input.
              </p>
                <p class="text-black-100" style="text-align: justify;" >
                We have implemented the live recognition by the other web server called Flask, it is powerful to run the python back-end. It offers opportunity to see how the future CCTV would looks like. It can detect the people, item and behaviours non-stopping and accurately.
                </p>
                <p class="text-black-100" style="text-align: justify;" >
                The output video outlines the top5 most significant items within the frame with its name. It can also catch up the faces in the image/video and if the machine is confident with the result, it will show who it is.
                </p>
                <p class="text-black-100" style="text-align: justify;" >
                For the posture recognition, it would draw the human body on the frame and brings the posture result in the form of text with the system time.
                </p>
              </div>
            </div>
          </section>


          <section id="page_flow" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Page Flow</h1>
                <p class="line"></p>
                <img src="img/pageflow.png">
                <p class="text-black-100" style="text-align: justify;"></br>The structure of our web application is demonstrated in the page flow diagram shown above. By clicking the start button
                on the homepage you will be taken to a menu in order to choose the form of input. After that the same process is followed on all inputs. Simply selecting the file you want to run the
               Recognition API on, and click the analyse button. The output is printed in the log area, with the output of each component of the Recognition API listed.</p>
              </div>
            </div>
          </section>


          <section id="sequence_diagram" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Sequence Diagram</h1>
                <p class="line"></p>
                <img src="img/sequence_diagram.png" class="sequence_diagram">
                <p class="text-black-100" style="text-align: justify;" >This is the sequence diagram for the local flask AI recognition, we do not have the sequence
                  diagram for the azure Apache AI recognition, the reason is that they are quite similar to each other, also the local machine sequence diagram covers
                   most of the cloud one but also with database, models and live stream features.
                  In that diagram, we shows how does the web application upload the media onto the local machine and stream the live frame to the backend web apps.
                  As an output, the results gained from the Google Cloud API for item recognition is the position of the top5 significant items within the frame with confidence rate.
                  The results gained from the face recognition is the face rectangle and the name of the likely people on the frame compared with the library. The posture recognition has
                  both the tensor detected by the frame and txt output with the posture guessing.</p>
              </div>
            </div>
          </section>


          <section id="design_pattern" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Design Patterns</h1>
                <p class="line"></p>
                <p class="text-black-100" style="text-align: justify;" >
                  The design pattern is that the website should be very flxible to use for all of the reuiqres of the AI recognition. It accepts
                  video uploading for recognition, image taken immediately for recognition and also the live stream taken by web camera immediately.
                </p>
                <p class="text-black-100" style="text-align: justify;" >
                  The output of the recognition should also be obvious and in different forms like text or image sketching etc. It helps differnet groups
                  of people to understand how fancy the technology it is, even readble by a child. That is the core of the whole system design
                </p>
              </div>
            </div>
          </section>

          <section id="implementation" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Implementation of Key functionalities</h1>
                <p class="line"></p>
                <h2 class="section-heading text-uppercase"><u>Posture Recognition</u></h2>
                <br>
                <p class="text-black-100" style="text-align: justify;" >This SDK consists of the followings:</p>
                  <ul class="text-black-100" style="text-align: justify;">
                    <li>OpenPose</li>
                    <li>TensorFlow</li>
                    <li>Tf-pose-estimation</li>
                    <li>Scikit-learn</li>
                    <li>MLP Classifier</li>
                    <li>AdaBoost Classifier</li>
                    <li>Gradient Boosting Trees</li>
                  </ul>


                <div class="row">
                  <div class="column">
                    <img class="sketch" src="img/design/ModelTraining.png" align="right" style="width: 100%;">
                  </div>
                  <div class="column">
                    <p class="text-black-100" style="text-align: justify;" >This SDK allows the users to train the machine to perform posture recognition by the users. As you can see from the left, the process consists of three parts: gathering samples, generating datasets and producing models via the classifiers.
                      <br>
                      <br>
                    Firstly, We gathered more than 15 images displaying sitting, standing and laying of a human separately. The reason why we need more than 15 images is because of the classifers that we are going to use after generating the datasets require more than 15 samples in order to perform machine learning. In details, We gathered the photos regarding three human actions mentioned from the opensource photos website and taking pictures of us. After that, we have to rename the photos file into ex. sitting1.jpg, sitting2.jpg, standing1.jpg and laying1.jpg.
                    </p>
                  </div>
                </div>
                <br>

                <p class="text-black-100" style="text-align: justify;" >Secondly, once the samples are gathered, we use a python file using OpenPose and Tf-pose-estimation to extract the (x,y) coordinates of 18 key points on the body and label them that what posture it is related to. After that, the datasets file in CSV format is generated.</p>

                <p class="text-black-100" style="text-align: justify;">Finally, we will then apply one of the classifiers such as Multilayer Perception Classifier in python using Sklearn to produce labels from data generated by OpenPose. The model is then generated in pkl format.</p>
                <br>
                <br>
                <div class="row">
                  <div class="column">
                    <p class="text-black-100" style="text-align: justify;" >But how can we use the model to process the posture recognition on the image, video and the live-feed video?
                      <br>
                      <br>
                    As you can see the architecture displayed on the right, the inputs can be analysed are image in jpg, video in mp4 and webcam. The model we created in previous session can be embedded into the executable python files, these files will run OpenCV gathered with Tf-pose-estimation to render the frame and analysis the human actions according the classification generated. Finally, the decision of whether the person inside the frame is sitting, standing or laying will be written inside the log file in txt.
                    </p>
                  </div>
                  <div class="column">
                    <img class="sketch" src="img/design/PostureProcess.png" align="right" style="width: 100%;">
                  </div>
                </div>
                <br>
                <br>
                <h2 class="section-heading text-uppercase"><u>Webcam Snapshot</u></h2>
                <br>
                <p class="text-black-100" style="text-align: justify;" >This is a functionality from the webapp in Azure, and it is separated into two parts: webcam display and snapshot.
                  <br>
                  <br>
                Navigator.mediaDevices is a JS library which allows the webapp to get permission from the end user in order to display the webcam of the end user. Due to the issues from the security and the web browser, the end user can use Firefox to display the webcam from local and online webserver, however, Google Chrome can only achieve it through local webserver.
                  <br>
                  <br>
                The webapp takes the snapshot of the webcam by drawing the canvas of the video tag of the webcam component. Once the canvas is drawn, then it saves and encode the canvas data as jpg, and send the data to a php file through JQuery. Finally, the php file will then decode the data and save it into the Linux FileSystem directory pointed.
                </p>
                <br>
                <br>
                <h2 class="section-heading text-uppercase"><u>AzureVM Integration</u></h2>
                <br>
                <p class="text-black-100" style="text-align: justify;" >As we mentioned that this functionality is the webapp including image, webcam snapshot and video to be processed by the image recognition. Due to the requirement of the project is simply developing the CCTV AI but implementing any sorts of webapp, we put Apache webserver, the image recognition package and object based files together into the AzureVM for simplicity.
                  <br>
                  <br>
                We used the GPU based virtual machine to host everything, this is because the posture recognition requires high performance on the graphic rendering. In order to process and identify the three types recognitions in each frame in a video quickly, we used K80 GPU to process our application in Azure. On the other hand, we also use it to train our model, however, we have to resize it to a higher performance specs(4 x K80, 224GB RAM) in order to process large quantity of images quickly. Once we trained everything, we lowered the specs of the virtual machine.
                  <br>
                  <br>
                For this particular webapp, we use PHP and JS to implement the front-end, webcam functionality, files upload and connect the Image Recognition. From the System Architecture, the key of how the webapp execute the Image recognition is by using <i>shell_exec()</i>. In addition, this command allows to perform shell command onto the virtual machine, hence we can use it to use the python environment generated from Anaconda to execute the python files from the Image Recognition. For example:
                  <br>
                </p>
                <p class="text-black-100" style="text-align: center;"><i>$posture_output = shell_exec('/var/www/anaconda3/envs/posture/bin/python /var/www/sapient/Posture/featureExtraction/tf-pose-estimation/single_image.py --image=/var/www/file.jpg');</i></p>
                  <br>
                <p class="text-black-100" style="text-align: justify;" >As you can see from this code, this is used for execute the Posture Recognition on the image uploaded via Anaconda python environment we generated. And then, we used the same method to execute the combination of Face and Item Recognition. After we gathered all the information generated from the Recognitions, we displayed the data of the variables onto the webapp.</p>

              </div>
            </div>
          </section>


        </div>
      </div>
    </div>





    <!-- Footer -->
    <footer class="bg-black small text-center text-white-50">
      <div class="container">
        Copyright &copy; Sapient Platform 2018
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/grayscale.min.js"></script>
    <script src="js/sidebar.js"></script>
  </body>

</html>
