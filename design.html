<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Sapient - CCTV Analysis AI</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/grayscale.min.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">
  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="index.html">SAPIENT</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="requirements.html">Requirements</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="research.html">Research</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="hci.html">HCI</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="design.html">Design</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="testing.html">Testing</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="evaluation.html">Evaluation</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="management.html">Management</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container d-flex h-100 align-items-center">
        <div class="mx-auto text-center">
          <h1 class="mx-auto my-0 text-uppercase">Design</h1>
          <h2 class="text-white-50 mx-auto mt-2 mb-5">Introducing Sapient's System Architecture</h2>
          <a href="#system_architecture" class="btn btn-primary js-scroll-trigger">Show me more</a>
        </div>
      </div>
    </header>

    <div class="container-fluid bg-light">
      <div class="row">
        <div class="col-sm-2">
          <br>
          <br>
          <div id="sidebar" class="sidebar">
           <nav class="navbar navbar-shrink">
              <ul class="navbar-nav">
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#system_architecture">System Architecture</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#page_flow">Page Flow</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#sequence_diagram">Sequence Diagram</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#design_pattern">Design Patterns</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#implementation">Implementation of key functionalities</a>
                </li>
              </ul>
            </nav>
          </div>
        </div>

        <div class="col-sm-10">

          <section id="system_architecture" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">System Architecture</h1>
                <p class="line"></p>
                <p class="text-black-100" style="text-align: justify;" ><b>- Azure Cloud:</b></p>
                <p class="text-black-100" style="text-align: justify;" >It is the platform which provides virtual machine allowing us to deploy our application and route the website to the world.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Linux Virtual Machine:</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is the machine we rent from Azure, which allows us to have a powerful GPU to run our image recogitions and route the website to the world by opening port 80</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Image Recognition</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is a package including the posture, item and face recognition.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Posture Recognition</b></p>
                <p class="text-black-100" style="text-align: justify;" >It uses openpose and tensorflow to perform the posture and human detection in a frame and machine learning of recognising standing, sitting and laying.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Face Recognition</b></p>
                <p class="text-black-100" style="text-align: justify;" >Description</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Item Recognition</b></p>
                <p class="text-black-100" style="text-align: justify;" >Description</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Apache</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is the webserver which mainly runs PHP, and our web application is mainly written in PHP. In addition, it has a high compatibility to access the Linux server and modify the filesystem in order to execute the image recognition.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- Object Storage Directory</b></p>
                <p class="text-black-100" style="text-align: justify;" >This is the directory in the Linux Filesystem, and it stores the input of images and videos from the user and the output of log files and videos from the image recognition.</p>
                <br>
                <p class="text-black-100" style="text-align: justify;" ><b>- User</b></p>
                <p class="text-black-100" style="text-align: justify;" >The user can upload their images, snapshot from webcam and video to the webapp and analyse the inputs through the image recognition.</p>

              </div>
            </div>
          </section>


          <section id="page_flow" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Page Flow</h1>
                <p class="line"></p>
                <img src="img/pageflow.png">
                <p class="text-black-100" style="text-align: justify;"></br>The structure of our web application is demonstrated in the page flow diagram shown above. By clicking the start button
                on the homepage you will be taken to a menu in order to choose the form of input. After that the same process is followed on all inputs. Simply selecting the file you want to run the
               Recognition API on, and click the analyse button. The output is printed in the log area, with the output of each component of the Recognition API listed.</p>
              </div>
            </div>
          </section>


          <section id="sequence_diagram" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Sequence Diagram</h1>
                <p class="line"></p>
                <p class="text-black-100" style="text-align: justify;" >Description</p>
              </div>
            </div>
          </section>


          <section id="design_pattern" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Design Patterns</h1>
                <p class="line"></p>
                <p class="text-black-100" style="text-align: justify;" >Description</p>
              </div>
            </div>
          </section>

          <section id="implementation" class="projects-section bg-light">
            <div class="container">
              <div class="col-lg-12 text-center">
                <h1 class="section-heading text-uppercase">Implementation of Key functionalities</h1>
                <p class="line"></p>
                <h2 class="section-heading text-uppercase"><u>Posture Recognition</u></h2>
                <br>
                <p class="text-black-100" style="text-align: justify;" >This SDK consists of the followings:</p>
                  <ul class="text-black-100" style="text-align: justify;">
                    <li>OpenPose</li>
                    <li>TensorFlow</li>
                    <li>Tf-pose-estimation</li>
                    <li>Scikit-learn</li>
                    <li>MLP Classifier</li>
                    <li>AdaBoost Classifier</li>
                    <li>Gradient Boosting Trees</li>
                  </ul>


                <div class="row">
                  <div class="column">
                    <img class="sketch" src="img/design/ModelTraining.png" align="right" style="width: 100%;">
                  </div>
                  <div class="column">
                    <p class="text-black-100" style="text-align: justify;" >This SDK allows the users to train the machine to perform posture recognition by the users. As you can see from the left, the process consists of three parts: gathering samples, generating datasets and producing models via the classifiers.
                      <br>
                      <br>
                    Firstly, We gathered more than 15 images displaying sitting, standing and laying of a human separately. The reason why we need more than 15 images is because of the classifers that we are going to use after generating the datasets require more than 15 samples in order to perform machine learning. In details, We gathered the photos regarding three human actions mentioned from the opensource photos website and taking pictures of us. After that, we have to rename the photos file into ex. sitting1.jpg, sitting2.jpg, standing1.jpg and laying1.jpg.
                    </p>
                  </div>
                </div>
                <br>

                <p class="text-black-100" style="text-align: justify;" >Secondly, once the samples are gathered, we use a python file using OpenPose and Tf-pose-estimation to extract the (x,y) coordinates of 18 key points on the body and label them that what posture it is related to. After that, the datasets file in CSV format is generated.</p>

                <p class="text-black-100" style="text-align: justify;">Finally, we will then apply one of the classifiers such as Multilayer Perception Classifier in python using Sklearn to produce labels from data generated by OpenPose. The model is then generated in pkl format.</p>
                <br>
                <br>
                <div class="row">
                  <div class="column">
                    <p class="text-black-100" style="text-align: justify;" >But how can we use the model to process the posture recognition on the image, video and the live-feed video?
                      <br>
                      <br>
                    As you can see the architecture displayed on the right, the inputs can be analysed are image in jpg, video in mp4 and webcam. The model we created in previous session can be embedded into the executable python files, these files will run OpenCV gathered with Tf-pose-estimation to render the frame and analysis the human actions according the classification generated. Finally, the decision of whether the person inside the frame is sitting, standing or laying will be written inside the log file in txt.
                    </p>
                  </div>
                  <div class="column">
                    <img class="sketch" src="img/design/PostureProcess.png" align="right" style="width: 100%;">
                  </div>
                </div>
                <br>
                <br>
                <h2 class="section-heading text-uppercase"><u>Webcam Snapshot</u></h2>
                <br>
                <p class="text-black-100" style="text-align: justify;" >This is a functionality from the webapp in Azure, and it is separated into two parts: webcam display and snapshot.
                  <br>
                  <br>
                Navigator.mediaDevices is a JS library which allows the webapp to get permission from the end user in order to display the webcam of the end user. Due to the issues from the security and the web browser, the end user can use Firefox to display the webcam from local and online webserver, however, Google Chrome can only achieve it through local webserver.
                  <br>
                  <br>
                The webapp takes the snapshot of the webcam by drawing the canvas of the video tag of the webcam component. Once the canvas is drawn, then it saves and encode the canvas data as jpg, and send the data to a php file through JQuery. Finally, the php file will then decode the data and save it into the Linux FileSystem directory pointed.
                </p>
                <br>
                <br>
                <h2 class="section-heading text-uppercase"><u>AzureVM Integration</u></h2>
                <br>
                <p class="text-black-100" style="text-align: justify;" >As we mentioned that this functionality is the webapp including image, webcam snapshot and video to be processed by the image recognition. Due to the requirement of the project is simply developing the CCTV AI but implementing any sorts of webapp, we put Apache webserver, the image recognition package and object based files together into the AzureVM for simplicity.
                  <br>
                  <br>
                We used the GPU based virtual machine to host everything, this is because the posture recognition requires high performance on the graphic rendering. In order to process and identify the three types recognitions in each frame in a video quickly, we used K80 GPU to process our application in Azure. On the other hand, we also use it to train our model, however, we have to resize it to a higher performance specs(4 x K80, 224GB RAM) in order to process large quantity of images quickly. Once we trained everything, we lowered the specs of the virtual machine.
                  <br>
                  <br>
                For this particular webapp, we use PHP and JS to implement the front-end, webcam functionality, files upload and connect the Image Recognition. From the System Architecture, the key of how the webapp execute the Image recognition is by using <i>shell_exec()</i>. In addition, this command allows to perform shell command onto the virtual machine, hence we can use it to use the python environment generated from Anaconda to execute the python files from the Image Recognition. For example:
                  <br>
                </p>
                <p class="text-black-100" style="text-align: center;"><i>$posture_output = shell_exec('/var/www/anaconda3/envs/posture/bin/python /var/www/sapient/Posture/featureExtraction/tf-pose-estimation/single_image.py --image=/var/www/file.jpg');</i></p>
                  <br>
                <p class="text-black-100" style="text-align: justify;" >As you can see from this code, this is used for execute the Posture Recognition on the image uploaded via Anaconda python environment we generated. And then, we used the same method to execute the combination of Face and Item Recognition. After we gathered all the information generated from the Recognitions, we displayed the data of the variables onto the webapp.</p>

              </div>
            </div>
          </section>


        </div>
      </div>
    </div>





    <!-- Footer -->
    <footer class="bg-black small text-center text-white-50">
      <div class="container">
        Copyright &copy; Sapient Platform 2018
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/grayscale.min.js"></script>
    <script src="js/sidebar.js"></script>
  </body>

</html>
